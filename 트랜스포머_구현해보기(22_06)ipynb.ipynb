{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "트랜스포머 구현해보기(22.06)ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- Reference : https://cpm0722.github.io/pytorch-implementation/transformer"
      ],
      "metadata": {
        "id": "9o-YbQjAnj7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Transformer"
      ],
      "metadata": {
        "id": "JByf-diR49dM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transformer의 개괄적인 구조"
      ],
      "metadata": {
        "id": "rSHIiZlk7I6R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa6UsF2D43zc"
      },
      "outputs": [],
      "source": [
        "# encoder 구현\n",
        "# decoder 구현\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x, z): # (?) x, z 는 각각 뭘까?\n",
        "        c = self.encoder(x)\n",
        "        y = self.decoder(z, c)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Encoder"
      ],
      "metadata": {
        "id": "SIWi53Cq5Ao1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Encoder"
      ],
      "metadata": {
        "id": "aUDNGIov7OG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder layer에서 input과 output의 shape는 동일하다 (그렇게 설계해야만 함)\n",
        "# input sentence 문장의 embedding (= 전체 Encoder의 input) -> Layer 1을 통과한 output -> Layer 2를 통과 \n",
        "# -> Layer 3을 통과 -> ... -> Layer N을 통과 -> context (= 전체 Encoder의 output)\n",
        "# 논문에서는 N = 6 사용\n",
        "# Encoder를 여러 Layer 겹쳐 쌓는 이유?\n",
        "# 각 Encoder Layer의 역할은, input으로 들어오는 vector에 대해 더 높은 차원(더 넓은 관점, 더 추상적인 정보로)에서의 context를 담는 것\n",
        "# 겹겹이 쌓이면서 점저 더 높은 차원의 context가 저장되게 됨\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, n_layer): # n_layer: Encoder Layer의 개수\n",
        "        super(Encoder, self).__init__()\n",
        "        # (?) 이 부분 nn.ModuleList(?) 써서 개선할 수 있지 않을까\n",
        "        self.layers = []\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(copy.deepcopy(encoder_layer))\n",
        "\n",
        "          \n",
        "    def forward(self, x):\n",
        "        out = x # 초기화 후 반복 (= Encoder 전체의 input)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out)\n",
        "        return out # (= Encoder 전체의 output (: context) )"
      ],
      "metadata": {
        "id": "MEkjxHNY6BuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Encoder Layer"
      ],
      "metadata": {
        "id": "yr1i8510EkSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder Layer 는 크게 Multi-Head Attention Layer 과 Position-wise-Feed-Forward Layer 로 구성됨\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, multi_head_attention_layer, position_wise_feed_forward_layer):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.multi_head_attention_layer = multi_head_attention_layer\n",
        "        self.position_wise_feed_forward_layer = position_wise_feed_forward_layer\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.multi_head_attention_layer(x)\n",
        "      out = self.position_wise_feed_forward_layer(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "bV-cHLEb8yOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Self-Attention\n",
        "\n",
        "  (= 논문에서는 'Scaled-Dot-Attention'이라고 표현함)"
      ],
      "metadata": {
        "id": "4Yzw0AZ-azUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 model에 들어오는 input은 한 개의 문장이 아니라 mini-batch이기 때문에 Q, K, V의 shape가 n_batch×seq_len×dk 이다 !!\n",
        "\n",
        "def calculate_attention(self, query, key, value, mask):\n",
        "    # query, key, value's shape: (n_batch, seq_len, \"d_k\")\n",
        "    # (query, key, value는 서로 다른 FC Layer를 거쳐 n_batch×max_seq_len×dk로 변형되었다.)\n",
        "\n",
        "    # mask : pad mask matrix (값이 0: pad token, 값이 1: 문장 내 기존 토큰)\n",
        "    # (pad mask matrix는 Transformer 외부 (대개 Batch class)에서 생성되어 Transformer에 인자로 들어오게 된다.)\n",
        "\n",
        "    # 준비 단계에서 FC_Layer를 통해 구한 Key -> 의 (행, 열) 크기 중 '열'이 dimension_k\n",
        "    d_k = key.size(-1) # (?) key.shape[-1] 로 써도 같은 값이 나올까??\n",
        "\n",
        "    # Q x K^T, attention_score's shape: (n_batch, seq_len, seq_len)\n",
        "    attention_score = torch.matmul(query, key.transpose(-2, -1))\n",
        "\n",
        "    # scaling\n",
        "    attention_score = attention_score / math.sqrt(d_k)\n",
        "\n",
        "    # (Optional) Pad Masking\n",
        "    if mask is not None:\n",
        "        attention_score = score.masked_fill(mask == 0, -1e9) # (?)score가 아니라 attention_score 아닌가?\n",
        "\n",
        "    # softmax, attention_prob's shape: (n_batch, seq_len, seq_len)\n",
        "    attention_prob = F.softmax(score, dim=-1)\n",
        "\n",
        "    # Attention_Prob x V, out's shape: (n_batch, seq_len, d_k)\n",
        "    out = torch.matmul(attention_prob, value)\n",
        "\n",
        "    return out # (n_batch, seq_len, d_k)\n"
      ],
      "metadata": {
        "id": "0WZ6Zvtga3Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Multi-Head Attention Layer"
      ],
      "metadata": {
        "id": "ZwnLT4rlQRPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, d_model, h, qkv_fc_layer, fc_layer):\n",
        "        # qkv_fc_layer's shape : (d_embed, \"d_model\")\n",
        "        #                                   => (중요) 단순히 d_k 가 아닌, d_model 로 Q,K,V 를 생성해서\n",
        "        #                                            -> 한 번의 self-attention 연산으로 output 계산이 가능하도록 만든다!\n",
        "        # fc_layer's shape : (d_model, d_embed)\n",
        "\n",
        "        super(MultiHeadAttentionLayer, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "\n",
        "        # [1] Q, K, V 생성을 위한 FC Layer 선언 (d_embed, d_model)\n",
        "        # (deepcopy 를 사용해 -> 3가지가 각 서로 다른 weight를 갖고 별개로 연산될 수 있도록 복사하여 선언함)\n",
        "        self.query_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
        "        self.key_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
        "        self.value_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
        "\n",
        "        # [2] 멀티헤드 attention 계산 이후, 거쳐가는 FC Layer (d_model, d_embed)\n",
        "        self.fc_layer = fc_layer\n",
        "\n",
        "\n",
        "    # (+) Decoder Layer에서는 query, key, value 가 서로 다른 embedding에서 넘어온다 !\n",
        "    #     따라서, 인자로 query, key, value를 모두 별개로 인풋해 주는 것\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # query, key, value's shape : (n_batch, seq_len, d_embed)\n",
        "        # (<- Input Sentence의 Q, k, V 차원)\n",
        "        # mask's shape : (n_batch, seq_len, seq_len)\n",
        "\n",
        "        n_batch = query.shape[0] # get n_batch\n",
        "\n",
        "        \n",
        "        # FC Layer 통과하면서 + 차원 변형해줌\n",
        "        def transform(x_input, fc_layer_to_transform): # reshape (n_batch, seq_len, d_embed) to (n_batch, seq_len, d_k)\n",
        "            out = fc_layer_to_transform(x_input)\n",
        "            # out's shape : (n_batch, seq_len, d_model)\n",
        "\n",
        "            out = out.view(n_batch, -1, self.h, self.d_model//self.h)\n",
        "            # out's shape : (n_batch, seq_len, \"h\", d_k)\n",
        "            # (\"view\" 함수의 사용용도 : 붙어있는 차원을 떼어낼 때 쓴다)\n",
        "            # (\"-1\" 값의 의미 : 자동으로 변환이 가능한 차원이 지정되어 차원 배정이 이루어짐 -> 2번째 차원 \"seq_len\"은 자동 지정된 것임)\n",
        "\n",
        "            out = out.transpose(1, 2).contiguos()\n",
        "            # out's shape : (n_batch, \"h\", seq_len, d_k)\n",
        "            # (?) \"view\" 함수는 contiguous한 tensor를 반환하고, \"transpose\" 함수는 non-contiguous한 tensor를 반환하는데,\n",
        "            #     여기서 코드를 \"transpose(1, 2).contiguous()\"로 안 써주어도 되나??\n",
        "            #     (다음에 이어서 view 등 함수를 쓸 일이 있으면 이렇게 해주는게 안전한데, 앞으로 또 연산할 일이 잘 없어서 그냥 패스한 것 ..?)\n",
        "\n",
        "            return out\n",
        "            # 이러한 차원 transform 작업을 수행하는 이유는 ?\n",
        "            # : 위에서 작성했던 \"calculate_attention()\"이 input으로 받고자 하는 shape가\n",
        "            #   \"(n_batch×...×seq_len×dk)\"이기 때문이다.\n",
        "\n",
        "\n",
        "\n",
        "      query = transform(query, self.query_fc_layer) # [1] Q, K, V 생성을 위한 FC Layer\n",
        "      key = transform(key, self.key_fc_layer) # [1] Q, K, V 생성을 위한 FC Layer\n",
        "      value = transform(value, self.key_fc_layer) # [1] Q, K, V 생성을 위한 FC Layer\n",
        "\n",
        "\n",
        "      if mask not None:\n",
        "          # 이전 mask's shape : (n_batch, seq_len, seq_len)\n",
        "          mask = mask.unsqueeze(1)\n",
        "          # 결과 mask's shape : (n_batch, \"1\", seq_len, seq_len)\n",
        "      \n",
        "      # (?) 아래에서 시행하는 \"calculate_attention\" 함수 내에서 \"어텐션 스코어\"를 구할때 mask_fill 함수를 수행하는 부분이 있음\n",
        "      #      그런데, 왜 mask 인자를 인풋하기 이전에 차원을 늘려서 바꾸어주어야만 할까 ?\n",
        "      #         : (n_batch×seq_len×seq_len) 형태를 (n_batch×1×seq_len×seq_len)로 변경하게 된다.\n",
        "      #           이는 calculate_attention() 내에서 masking을 수행할 때 broadcasting이 제대로 수행되게 하기 위함이다.\n",
        "      #           -> transform 함수와 마찬가지로 위에서 작성했던 \"calculate_attention()\"이 input으로 받고자 하는 shape가\n",
        "      #               \"(n_batch×...×seq_len×dk)\"이기 때문이다. (?)\n",
        "\n",
        "\n",
        "      out = self.calculate_attention(query, key, value, mask)\n",
        "      # out's shape: (n_batch, h, seq_len, d_k)\n",
        "\n",
        "      out = out.transpose(1, 2)\n",
        "      # out's shape: (n_batch, seq_len, h, d_k)\n",
        "\n",
        "      out = contiguous().view(n_batch, -1, self.d_model)\n",
        "      # out's shape: (n_batch, seq_len, \"d_model\")\n",
        "      #                                 * d_model 값 = (h * d_k)\n",
        "      # 차원 합쳐주는 이유?\n",
        "      #     => \"멀티-헤드\" 어텐션이기 때문에 ?\n",
        "      #       h(=8) 만큼 dimension을 합쳐서 Q,K,V vector를 생성해주어야 -> 한 번의 self-attention으로 연산이 수행 가능하다 !\n",
        "      #       (h(=8) 만큼의 연산 효과를 수행하는 이유는, 여러 \"attention\" 정보를 반영함으로써, 더 많은 \"정보\"를 담아내기 위함이다 !)\n",
        "\n",
        "      out = self.fc_layer(out) # [2] 멀티헤드 attention 계산 이후, 거쳐가는 FC Layer\n",
        "                                # (d_model -> d_embed 로 차원 변경의 효과 있음)\n",
        "      # out's shape: (n_batch, seq_len, \"d_embed\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Input Sentence의 Q, k, V 차원 : ( n_batch, seq_len, d_embed )\n",
        "# -> [1] Q, K, V 생성을 위한 FC Layer 통과 : (d_embed -> d_model 변환시키는 weight matrix임)\n",
        "# 그 결과, 생성된 Q, K, V 의 크기는 : ( n_batch, seq_len, d_\bk )\n",
        "\n",
        "# <멀티헤드 어텐션> 수행 ...\n",
        "\n",
        "# -> [2] 멀티헤드 attention 계산 이후, 거쳐가는 FC Layer 통과 (d_model -> d_embed 로 다시 변환시키는 weight matrix임)\n",
        "# (\"멀티 헤드 어텐션\" 전체 연산(즉, 하나의 큰 함수)에서의 input과 output 크기를 맞춰주기 위함)"
      ],
      "metadata": {
        "id": "whyhYiW_QT3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Encoder Layer (수정)"
      ],
      "metadata": {
        "id": "CjECitS2woaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, multi_head_attention_layer, position_wise_feed_forward_layer):\n",
        "      super(EncoderLayer, self).__init__()\n",
        "      self.multi_head_attention_layer = multi_head_attention_layer\n",
        "      self.position_wise_feed_forward_layer = position_wise_feed_forward_layer\n",
        "\n",
        "\n",
        "  def forward(self, x, mask): # mask 인자 추가\n",
        "      out = self.multi_head_attention_layer(query=x, key=x, value=x, mask=mask) # 입력 인자 : (x, x, x, mask)\n",
        "      # 앞의 multi_head_attention_layer 에서는 인자가 1개(x)일 것으로 가정하고 코드를 구현하였음\n",
        "      # but, 실제로는 query, key, value를 받아야하므로 이를 수정해준다 ! + mask 역시 인자로 받는다 !\n",
        "      out = self.position_wise_feed_forward_layer(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "oSB5LPLddOKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Encoder (수정)"
      ],
      "metadata": {
        "id": "ShNKyabN0bCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, n_layer): # n_layer: Encoder Layer의 개수\n",
        "        super(Encoder, self).__init__()\n",
        "        # (?) 이 부분 nn.ModuleList(?) 써서 개선할 수 있지 않을까\n",
        "        self.layers = []\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(copy.deepcopy(encoder_layer))\n",
        "\n",
        "          \n",
        "    def forward(self, x, mask): # mask 인자 추가\n",
        "        out = x # 초기화 후 반복 (= Encoder 전체의 input)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, mask)\n",
        "        return out # (= Encoder 전체의 output, context)"
      ],
      "metadata": {
        "id": "v0-DDY2W0crQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transformer (수정)"
      ],
      "metadata": {
        "id": "NFkZFs681RQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x, z): # (?) x, z 는 각각 뭘까?\n",
        "                                  # x : src\n",
        "                                  # z : trg\n",
        "        encoder_output = self.encoder(x, mask)\n",
        "        out = self.decoder(z, encoder_output)\n",
        "        return out"
      ],
      "metadata": {
        "id": "UFngmfAL1TUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Position-wise Feed Forward Layer"
      ],
      "metadata": {
        "id": "gvCI0HBz50mS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed Forward Layer는 단순하게 2개의 FC Layer를 갖는 레이어\n",
        "\n",
        "class PositionWiseFeedForwardLayer(nn.Module):\n",
        "    def __init__(self, first_fc_layer, second_fc_layer):\n",
        "        self.first_fc_layer = first_fc_layer\n",
        "        self.second_fc_layer = second_fc_layer\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.first_fc_layer(x)\n",
        "      # 첫 번째 FC Layer의 weight matrix 크기 : (d_embed x d_ff)\n",
        "      out = F.relu(out)\n",
        "      out = self.dropout(out) # (이 과정에서 dropout 도 추가)\n",
        "      out = self.second_fc_layer(out)\n",
        "      # 두 번째 FC Layer의 weight matrix 크기 : (d_ff x d_embed)\n",
        "      return out # 이를 통해 input과 같은 shape으로 output의 shape 유지가 가능"
      ],
      "metadata": {
        "id": "Bq6kM_ZR53oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Norm Layer (Residual Connection)"
      ],
      "metadata": {
        "id": "PH6jFf3BDpej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnectionLayer(nn.Module):\n",
        "    def __init__(self, norm_layer):\n",
        "        super(ResidualConnectionLayer, self).__init__()\n",
        "        self.norm_layer = norm_layer\n",
        "\n",
        "\n",
        "    def forward(self, x, sub_layer):\n",
        "        out = sub_layer(x) + x # Residual Connection 적용\n",
        "        out = self.norm_layer(out) # Layer Normalization 추가 적용\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "7kyOZzvGDtNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Encoder Layer (수정 2)"
      ],
      "metadata": {
        "id": "Oe20edY1EVXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, multi_head_attention_layer, position_wise_feed_forward_layer):\n",
        "      super(EncoderLayer, self).__init__()\n",
        "      self.multi_head_attention_layer = multi_head_attention_layer\n",
        "      self.position_wise_feed_forward_layer = position_wise_feed_forward_layer\n",
        "      self.residual_connection_layers = [ResidualConnectionLayer(copy.deepcopy(norm_layer)) for i in range(2)]\n",
        "      # residual_connection_layers에 ResidualConnectionLayer를 2개 생성해 저장하고,\n",
        "\n",
        "\n",
        "  def forward(self, x, mask): # mask 인자 추가\n",
        "      '''\n",
        "      out = self.multi_head_attention_layer(query=x, key=x, value=x, mask=mask) # 입력 인자 : (x, x, x, mask)\n",
        "      # 앞의 multi_head_attention_layer 에서는 인자가 1개(x)일 것으로 가정하고 코드를 구현하였음\n",
        "      # but, 실제로는 query, key, value를 받아야하므로 이를 수정해준다 ! + mask 역시 인자로 받는다 !\n",
        "      out = self.position_wise_feed_forward_layer(out)\n",
        "      '''\n",
        "\n",
        "      # 0번째는 multi_head_attention_layer를 감싸고, 1번째는 position_wise_feed_forward_layer를 감싸게 된다.\n",
        "      # <ResidualConnectionLayer 클래스의 forward 함수의 인자> 는\n",
        "      # forward(self, x, sub_layer)\n",
        "      out = self.residual_connection_layers[0](x, lambda x: self.multi_head_attention_layer(query=x, key=x, value=x, mask=mask))\n",
        "      out = self.residual_connection_layers[1](x, lambda x: self.position_wise_feed_forward_layer(x))\n",
        "      # (?) 헷갈림\n",
        "      return out"
      ],
      "metadata": {
        "id": "LbYE-0t5EW3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Decoder"
      ],
      "metadata": {
        "id": "B961KrJY44-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Teacher Forcing in Transformer (Subsequent Masking)"
      ],
      "metadata": {
        "id": "SXtpmNsj5OLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`make_std_mask()는 subsequent_mask()를 호출해 subsequent mask을 생성하고, 이를 pad mask와 결합한다. 위의 code는 Transformer 내부가 아닌 Batch class 내에서 실행되는 것이 바람직할 것이다. mask 생성은 Transformer 내부 작업이 아닌 전처리 과정에 포함되기 때문이다. 따라서 Encoder에 적용되는 pad mask와 동일하게 Batch class 내에서 생성될 것이다.`"
      ],
      "metadata": {
        "id": "Sqlm3h7i73ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer가 RNN에 비해 갖는 가장 큰 장점은 병렬 연산이 가능하다는 것이었다.\n",
        "# 병렬 연산을 위해 ground truth의 embedding을 matrix로 만들어 input으로 그대로 사용하게 되면,\n",
        "# Decoder에서 Self-Attention 연산을 수행하게 될 때 현재 출력해내야 하는 token의 정답까지 알고 있는 상황이 발생한다.\n",
        "# 따라서 masking을 적용해야 한다. i번째 token을 생성해낼 때, 1∼i−1의 token은 보이지 않도록 처리를 해야 하는 것이다.\n",
        "\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    atten_shape = (1, size, size)\n",
        "    mask = np.triu(np.ones(atten_shape), k=1).astype('uint8') # masking with upper triangle matrix\n",
        "    return torch.from_numpy(mask)==0 # reverse (masking=False, non-masking=True)\n",
        "\n",
        "\n",
        "def make_std_mask(tgt, pad): # subsequent_mask()를 호출해 subsequent mask을 생성하고, 이를 pad mask와 결합한다.\n",
        "    tgt_mask = (tgt != pad) # pad masking\n",
        "    tgt_mask = tgt_mask.unsqueeze(-2) # reshape (n_batch, seq_len) -> (n_batch, 1, seq_len)\n",
        "    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1))).type_as(tgt_mask.data) # pad_masking & subsequent_masking\n",
        "    return tgt_mask\n",
        "    # (??)\n",
        "\n"
      ],
      "metadata": {
        "id": "Pu-cXRWs5IMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transformer (수정 2)"
      ],
      "metadata": {
        "id": "SdpkML2v8Rew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "\n",
        "    # 기존에는 Encoder에서 사용하는 pad mask(src_mask)만이 forward()의 인자로 들어왔다면,\n",
        "    # 이제는 Decoder에서 사용할 subsequent mask (trg_mask)도 함께 주어진다.\n",
        "\n",
        "    # forward(인코더의 input, 디코더의 input, 인코더의 mask, 디코더의 mask)\n",
        "    # 인코더의 mask : 'src_mask' 인자 (= 일반적인 Encoder에서 사용하는 pad mask)\n",
        "    # 디코더의 mask : 'trg_mask' 인자 (= Decoder에서 사용할 subsequent mask)\n",
        "    def forward(self, src, trg, src_mask, trg_mask): # (?) x, z 는 각각 뭘까?\n",
        "                                  # x : src\n",
        "                                  # z : trg\n",
        "        encoder_output = self.encoder(src, src_mask) # src_mask 가 함께 인풋됨\n",
        "        out = self.decoder(trg, trg_mask, encoder_output) # trg_mask 가 함께 인풋됨\n",
        "        return out"
      ],
      "metadata": {
        "id": "hMisdl8v8Tfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decoder"
      ],
      "metadata": {
        "id": "Llxsw1BnnSGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, sub_layer, n_layer): # n_layer: Decoder Layer의 개수\n",
        "        super(Decoder, self).__init__()\n",
        "        # (?) 이 부분 nn.ModuleList(?) 써서 개선할 수 있지 않을까\n",
        "        self.layers = []\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(copy.deepcopy(sub_layer))\n",
        "\n",
        "          \n",
        "    def forward(self, x, mask, encoder_output, encoder_mask): # encoder_mask 와 encoder_mask 추가\n",
        "        # x : Decoder의 input sentence\n",
        "        # mask : Decoder의 subsequent mask\n",
        "        # encoder_output : Encoder의 Context\n",
        "        # encoder_mask : Encoder의 mask(: 일반적인 encoder에서 사용한 pad masking)\n",
        "\n",
        "        out = x # 초기화 후 반복 (= Encoder 전체의 input)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, mask, encoder_output, encoder_mask) # encoder_mask 와 encoder_mask 추가\n",
        "        return out # (= Decoder 전체의 output -> 이것은 무엇인가??)"
      ],
      "metadata": {
        "id": "0pCAb2IInTMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, masked_multi_head_attention_layer, multi_head_attention_layer, position_wise_feed_forward_layer, norm_layer):\n",
        "      super(DecoderLayer, self).__init__()\n",
        "      self.masked_multi_head_attention_layer = ResidualConnectionLayer(masked_multi_head_attention_layer, copy.deepcopy(norm_layer))\n",
        "      self.multi_head_attention_layer = ResidualConnectionLayer(multi_head_attention_layer, copy.deepcopy(norm_layer))\n",
        "      self.position_wise_feed_forward_layer = ResidualConnectionLayer(position_wise_feed_forward_layer, copy.deepcopy(norm_layer))\n",
        "\n",
        "      # (?) <ResidualConnectionLayer 클래스의 forward 함수의 인자> 는\n",
        "      # forward(self, x, sub_layer)\n",
        "      # 어떻게 x 값에 layer를 인풋해도 되는 것 ?????\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x, mask, encoder_output, encoder_mask): # mask 인자 추가\n",
        "      # 디코더는 '멀티-헤드 어텐션 레이어' 2개 + 'position-wise-feed-forward layer' 1개로 이루어져 있음\n",
        "\n",
        "      # (1) 1번 째 멀티-헤드 어텐션 레이어\n",
        "      \n",
        "      # <특징>\n",
        "      # \"subsequent mask\"가 적용된다.\n",
        "      # self-attention 을 수행하는 원리이다.\n",
        "      out = self.masked_multi_head_attention_layer(query=x, key=x, value=x, mask=mask)\n",
        "\n",
        "      # (2) 2 번 째 멀티-헤드 어텐션 레이어\n",
        "      \n",
        "      # <특징>\n",
        "      # 이전 레이어인 'masked-multi-head-attention layer'로부터 넘겨 받은 ouput => query로 쓰임 (teacher forcing 원리가 들어감)\n",
        "      # + Encoder에서 도출된 context (=encoder_output) => key, value로 쓰임\n",
        "      # self-attention이 아닌, 이 두 가지 input 사이의 attention을 계산하는 단계이다.\n",
        "\n",
        "      # (?) \"일반적인 pad masking (= 'encoder_output' 인자)\" 을 쓰는 이유 ?\n",
        "      #     : teacher forcing으로 넘어온 input(= 원하는 output과 유사함)을 query로 날려서\n",
        "      #       이에 상응하는 Encoder 에서의 attention을 계산해내기 때문에\n",
        "      #       -> 이때, 'encoder_output' 인자와 'encoder_mask' 인자가 필요하다 ?\n",
        "      out = self.multi_head_attention_layer(query=out, key=encoder_output, value=encoder_output, mask=encoder_mask)\n",
        "\n",
        "      out = self.position_wise_feed_forward_layer(x=out)\n",
        "\n",
        "      return out"
      ],
      "metadata": {
        "id": "Ie5FRMc7pWXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transformer (수정 2 와 동일)"
      ],
      "metadata": {
        "id": "VBiXXQc36nhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "\n",
        "    # 기존에는 Encoder에서 사용하는 pad mask(src_mask)만이 forward()의 인자로 들어왔다면,\n",
        "    # 이제는 Decoder에서 사용할 subsequent mask (trg_mask)도 함께 주어진다.\n",
        "\n",
        "    # forward(인코더의 input, 디코더의 input, 인코더의 mask, 디코더의 mask)\n",
        "    # 인코더의 mask : 'src_mask' 인자 (= 일반적인 Encoder에서 사용하는 pad mask)\n",
        "    # 디코더의 mask : 'trg_mask' 인자 (= Decoder에서 사용할 subsequent mask)\n",
        "    def forward(self, src, trg, src_mask, trg_mask): # (?) x, z 는 각각 뭘까?\n",
        "                                  # x : src\n",
        "                                  # z : trg\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, trg_mask, encoder_output, srg_mask) # encoder의 mask(= src_mask) 까지 함께 넘겨준다 !\n",
        "        return out"
      ],
      "metadata": {
        "id": "CjnkuBXf6rJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transformer's Input (Positional Encoding)"
      ],
      "metadata": {
        "id": "d9U47Zbt8ezB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer의 input은 [1] 단순한 sentence(\"token embedding sequence\")에 더해 [2] \"Positional Encoding\"이 추가되게 된다.\n",
        "# 전체 TransformerEmbedding은 [1]\"단순 Embedding\"과 [2]\"PositionalEncoding\"의 sequential이다.\n",
        "\n",
        "\n",
        "class TransformerEmbedding(nn.Module):\n",
        "    def __init__(self, embedding, positional_encoding):\n",
        "        super(TransformerEmbedding, self).__init__()\n",
        "        self.embedding = nn.Sequential(embedding, positional_encoding)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "SHQcyEur9ijw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [1] 단순한 sentence(\"token embedding sequence\")\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, d_embed, vocab):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), d_embed) \n",
        "        # 임베딩 벡터의 (행,열) 크기를 (vocab 크기, d_embed 차원 크기)로 하여, embedding을 생성해낸다.\n",
        "        self.vocab = vocab\n",
        "        self.d_embed = d_embed\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x) * math.sqrt(self.d_embed)\n",
        "        # 주목할 점은 embedding에도 scaling을 적용한다는 점이다. \n",
        "        # forward()에서 √(d_embed) 를 곱해주게 된다.\n",
        "        return out"
      ],
      "metadata": {
        "id": "JfLHzrU2-PBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [2] \"Positional Encoding\"\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_embed, max_seq_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        encoding = torch.zeros(max_seq_len, d_embed)\n",
        "        position = torch.arange(0, max_seq_len).unsqueeze(1) # -> 그 결과, 차원은 (0, 1, max_seq_len) 이 됨 ?\n",
        "        div_term = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed))\n",
        "        # torch.arange(0, d_embed, 2) 값 : tensor([0, 2, 4, 6, 8, .., +2, ..., (d_embed-1)])\n",
        "        encoding[:, 0::2] = torch.sin(position * div_term) # 짝수 index\n",
        "        encoding[:, 1::2] = torch.cos(position * div_term) # 홀수 index\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 구현 상에서 주의할 점은 forward() 내에서 생성하는 Variable이 학습이 되지 않도록 requires_grad=False 옵션을 부여해야 한다는 것이다.\n",
        "        # PositionalEncoding은 학습되는 parameter가 아니기 때문이다.\n",
        "\n",
        "        out = x + Variable(self.encoding[:, :x.size(1)], requires_grad=False)\n",
        "        # :x.size(1) -> 모든 행에 대해 수행하는데, input sequence(x)의 열 크기만큼만 자르는 의미?\n",
        "        out = self.dropout(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ekEfPKXf_23I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transformer (수정 3)"
      ],
      "metadata": {
        "id": "s_fvpGo1NfsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이렇게 생성해낸 embedding을 Transformer에 추가해주자.\n",
        "\n",
        "# src_embed와 trg_embed를 Transformer의 생성자 인자로 추가한다.\n",
        "# forward() 내부에서 Encoder와 Decoder의 forward()를 호출할 때 각각 \"src_embed(src)\"\", \"trg_embed(trg)\"와 같이 input을 TransformerEmbedding으로 감싸 변환해준다.\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_embed, trg_embed, encoder, decoder):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.src_embed = src_embed # 추가\n",
        "        self.trg_embed = trg_embed # 추가\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "\n",
        "\n",
        "    # 기존에는 Encoder에서 사용하는 pad mask(src_mask)만이 forward()의 인자로 들어왔다면,\n",
        "    # 이제는 Decoder에서 사용할 subsequent mask (trg_mask)도 함께 주어진다.\n",
        "\n",
        "    # forward(인코더의 input, 디코더의 input, 인코더의 mask, 디코더의 mask)\n",
        "    # 인코더의 mask : 'src_mask' 인자 (= 일반적인 Encoder에서 사용하는 pad mask)\n",
        "    # 디코더의 mask : 'trg_mask' 인자 (= Decoder에서 사용할 subsequent mask)\n",
        "    def forward(self, src, trg, src_mask, trg_mask): # (?) x, z 는 각각 뭘까?\n",
        "                                  # x : src\n",
        "                                  # z : trg\n",
        "\n",
        "        # 인코딩/디코딩 과정에서 각각 인코더/디코더 용 embedding 으로 src/trg 를 감싸준다 !                          \n",
        "        encoder_output = self.encoder(self.src_embed(src), src_mask)\n",
        "        out = self.decoder(self.trg_embed(trg), trg_mask, encoder_output, srg_mask) # encoder의 mask(= src_mask) 까지 함께 넘겨준다 !\n",
        "        return out"
      ],
      "metadata": {
        "id": "qEowB_faNi6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Generator"
      ],
      "metadata": {
        "id": "kcDKuMmpQnlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Decoder의 output이 그대로 Transformer의 최종 output이 되는 것은 아니다 !\n",
        "추가적인 layer(= Generator)를 거침\n",
        "\n",
        "\n",
        "Decoder의 output은 (n_batch X seq_len X d_model) 의 shape를 갖는 matrix\n",
        "이를 vocabulary 를 사용해 \"실제 token\"으로 대응해 변환가능하도록 차원을 수정해줘야 함\n",
        "즉, FC Layer를 거치면서 마지막 dimension \"d_model\" 을 -> \"len(vocab)\" 으로 변경하도록 구현\n",
        "\n",
        "이후 softmax(log_softmax 가 성능 더 높음) 함수를 사용해 각 vocabulary에 대한 확률값으로 변환\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ThWlR7NMW4Wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, src_embed, trg_embed, encoder, decoder, fc_layer):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_embed = src.embed\n",
        "        self.trg_embed = trg_embed\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.fc_layer = fc_layer\n",
        "\n",
        "\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        encoder_output = self.encoder(self.src_embed(src), src_mask)\n",
        "        out = self.decoder(self.trg_embed(trg), trg_mask, encoder_output, src_mask)\n",
        "        \n",
        "        out = self.fc_layer(out) # FC Layer를 통해 마지막 차원을 \"d_model\" -> \"len(vocab)\" 으로 변경해줌\n",
        "        out = F.log_softmax(out, dim=-1) # \"dim=-1\" 의 의미 ?\n",
        "                                          # : 마지막 dimension을 기준으로 log_softmax 함수를 통해 계산 ?\n",
        "                                          # : 마지막 dimension(= \"len(vocab)\"\")에 대한 확률값을 구해야 함\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "c3p-_kNvQqLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Make Model"
      ],
      "metadata": {
        "id": "Wl6t_kl6DYvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer를 생성하는 예제 함수 make_model()\n",
        "def make_model(\n",
        "      src_vocab,\n",
        "      trg_vocab,\n",
        "      d_embed = 512,\n",
        "      n_layer = 6,\n",
        "      d_model = 512,\n",
        "      h = 8,\n",
        "      d_ff = 2048):\n",
        "\n",
        "\n",
        "    cp = lambda x: copy.deepcopy(x)\n",
        "    # (?) cp 는 함수인 것 ?!\n",
        "\n",
        "\n",
        "    # multi_head_attention_layer 생성한 뒤 copy해 사용\n",
        "    multi_head_attention_layer = MultiHeadAttentionLayer(\n",
        "                                    d_model = d_model, \n",
        "                                    h = h, \n",
        "                                    qkv_fc_layer = nn.Linear(d_embed, d_model), \n",
        "                                    fc_layer = nn.Linear(d_model, d_embed))\n",
        "                                    \n",
        "\n",
        "    # position_wise_feed_forward_layer 생성한 뒤 copy해 사용\n",
        "    position_wise_feed_forward_layer = PositionWiseFeedForward(\n",
        "                                          first_fc_layer = nn.Linear(d_embed, d_ff), \n",
        "                                          second_fc_layer = nn.Linear(d_ff, d_embed))\n",
        "    \n",
        "\n",
        "    # norm_layer 생성한 뒤 copy해 사용\n",
        "    norm_layer = nn.LayerNorm(d_embed, eps=1e-6)\n",
        "\n",
        "\n",
        "    # 실제 model 생성\n",
        "    model = Transformer(\n",
        "                ##### <SRC embedding 생성> #####\n",
        "                # TransfomerEmbedding 클래스 : 전체 TransformerEmbedding은 [1]\"단순 Embedding\"과 [2]\"PositionalEncoding\"의 sequential이다.\n",
        "                src_embed = TransfomerEmbedding(\n",
        "                                # [1] 단순한 sentence(\"token embedding sequence\")\n",
        "                                embedding = Embedding(\n",
        "                                                d_embed = d_embed, \n",
        "                                                vocab = src_vocab), \n",
        "                                 \n",
        "                                # [2] \"Positional Encoding\"\n",
        "                                positional_encoding = PositionalEncoding(\n",
        "                                                          d_embed = d_embed), \n",
        "                                                          ###max_seq_len=5000) # PositionalEncoding 선언 시 default 값\n",
        "                                ), \n",
        "                        \n",
        "\n",
        "                ##### <TRG embedding 생성> #####\n",
        "                # TransfomerEmbedding 클래스 : 전체 TransformerEmbedding은 [1]\"단순 Embedding\"과 [2]\"PositionalEncoding\"의 sequential이다.\n",
        "                trg__embed = TransfomerEmbedding(\n",
        "                                # [1] 단순한 sentence(\"token embedding sequence\")\n",
        "                                embedding = Embedding(\n",
        "                                                d_embed = d_embed, \n",
        "                                                vocab = trg_vocab), \n",
        "                                 \n",
        "                                # [2] \"Positional Encoding\"\n",
        "                                positional_encoding = PositionalEncoding(\n",
        "                                                          d_embed = d_embed), \n",
        "                                                          ###max_seq_len=5000) # PositionalEncoding 선언 시 default 값\n",
        "                                ), \n",
        "                \n",
        "\n",
        "                ##### <Encoder 생성> #####\n",
        "                # Encoder 클래스 : encoder_layer/sub_layer 인자를 n_layer 인자만큼 레이터를 쌓아줌\n",
        "                encoder = Encoder(\n",
        "                                              # EncoderLayer 클래스 : Encoder Layer 는 크게 Multi-Head Attention Layer 과 Position-wise-Feed-Forward Layer 로 구성됨\n",
        "                              sub_layer = EncoderLayer(\n",
        "                                                  multi_head_attention_layer = cp(multi_head_attention), \n",
        "                                                  position_wise_feed_forward_layer = cp(position_wise_feed_forward_layer),\n",
        "                                                  norm_layer = cp(norm_layer)), # (?) 위에서는 EncoderLayer 클래스에 norm_layer 인자는 선언 안 되어 있음 ? \n",
        "                              n_layer = n_layer), \n",
        "                \n",
        "\n",
        "                ##### <Decoder 생성> #####\n",
        "                # Decoder 클래스 : decoder_layer/sub_layer 인자를 n_layer 인자만큼 레이터를 쌓아줌\n",
        "                decoder = Decoder(\n",
        "                              sub_layer = DecoderLayer(\n",
        "                                              masked_multi_head_attention_layer = cp(multi_head_attention_layer),\n",
        "                                              # DecoderLayer 내부 동작에서 \"masked_multi_~\" 가 돌아가는거지\n",
        "                                              # 인자로 인풋해줄 때에는 \"masked_multi_~\" 레이어와 \"multi_head_~\" 레이어에 둘다 같은 레이어(멀티-헤드 어텐션 레이어)를 넣어준다??\n",
        "                                              multi_head_attention_layer = cp(multi_head_attention_layer), \n",
        "                                              position_wise_feed_forward_layer = cp(position_wise_feed_forward_layer), \n",
        "                                              norm_layer = cp(norm_layer)), \n",
        "\n",
        "                              n_layer = n_layer),\n",
        "                              \n",
        "\n",
        "                ##### <Generator 생성> #####\n",
        "                # Generator의 FC Layer 생성\n",
        "                fc_layer = nn.Linear(d_model, len(trg_vocab)))"
      ],
      "metadata": {
        "id": "WgaZyF2kDapj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}